# -*- coding: utf-8 -*-
"""Nevi_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iFpOP-KlGstrHgbSM2mQ74gxnlL_obt8

#Progetto del 'Laboratorio di Intelligenza Artificiale': Nevi Classification

Progetto del laboraorio di Intelligenza Artificiale (Ingegneria Informatica e Automatica - Università La Sapienza di Roma).  

Il progetto consiste nella creazione di un classificatore con reti neurali in grado di discriminare un melanoma da un nevo benigno.  
Si è fatto riferimento alla Challenge proposta da ISIC (International Skin Imaging Collaboration - *https://challenge.isic-archive.com*) nel 2016, e in particolare alla task 3 (Lesion Classification - *https://challenge.isic-archive.com/landing/2016/39/*), usandone i dataset e testando il modello attraverso le stesse metriche.

Progetto di Spagnoli Valerio: *spagnoli.1887715@studenti.uniroma1.it*.  
  
[![View filled on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=purple)](https://github.com/ValerioSpagnoli/Nevi-Classification/blob/main/Nevi_Classification.ipynb)
[![Open filled In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1e4YtGOXGq-ZCI5Iun__t99Y4HTTMuQXa?authuser=1#scrollTo=uQbBFaezBVRa)

##Importazione delle librerie, collegamento a Google Drive, e definzione delle variabili
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim import optimizer
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, ConcatDataset, SubsetRandomSampler, random_split

import torchvision
from torchvision.utils import make_grid
from torchvision.models import resnet
from torchvision.models.resnet import ResNet18_Weights
from torchvision import transforms
from torchvision.transforms import AutoAugment, AutoAugmentPolicy
import torchaudio

import pandas as pd

import os

from skimage import io
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, precision_recall_curve, f1_score, roc_curve, roc_auc_score, average_precision_score, balanced_accuracy_score
from sklearn.model_selection import KFold

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid

import numpy as np
from numpy.core.fromnumeric import reshape

import seaborn as sns

import random

from google.colab import files

import warnings

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

batch_size = 32
num_epochs = 10
resolution = (800, 800)
learning_rate = 0.000001
classes = ('benign', 'malignant')

K=8
Kfolder = KFold(n_splits = K, shuffle = True)
fold_performance={}

"""##Classe del dataset, funzioni per statistiche del dataset e funzioni ausiliarie"""

class ISIC_Dataset(Dataset):
  def __init__(self, csv_file, img_dir, transform=None, data=None, device=None):
    self.img_labels = pd.read_csv(csv_file)
    self.img_dir = img_dir
    self.transform = transform
    self.data = data
    self.code_img = None
    self.device = device

  def __len__(self):
    return len(self.img_labels)

  def __getitem__(self, idx):
    self.code_img = self.img_labels.iloc[idx, 0]
    img_path = os.path.join(self.img_dir, self.code_img) + '.jpg'
    image = io.imread(img_path)

    #benign = 0 malignant = 1
    
    if(self.data == 'Train'):
      label_bm = self.img_labels.iloc[idx, 1]
      label = 0
      if(label_bm == 'malignant'): 
        label = 1

    elif(self.data == 'Test'):
      label = self.img_labels.iloc[idx, 1].astype(np.int64)


    if self.transform:
      image = self.transform(image)

    image.to(self.device)
  
    return image, label

def get_dataset(data_type = None, path_csv = None, path_img = None, batch_size = None):
  
  dataset = None

  transform = transforms.Compose([transforms.ToPILImage(),
                                  transforms.Resize(resolution),
                                  transforms.ToTensor(),
                                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])
  
  if data_type == "Train":
  
    dataset = ISIC_Dataset(csv_file = path_csv, img_dir = path_img, transform = transform, data = "Train", device = device)

  elif data_type == "Test":

    dataset = ISIC_Dataset(csv_file = path_csv, img_dir = path_img, transform = transform, data = "Test", device = device)
    
  if(dataset == None):
    print("Errore nel setup del dataset.")
    return

  return dataset

def get_aug_dataset():

  dataset = None

  path_csv = '/content/drive/MyDrive/Università/Progetto Lab IAGI/Dataset/Training/ISBI2016_ISIC_Part3_Training_GroundTruth.csv' 
  path_img = '/content/drive/MyDrive/Università/Progetto Lab IAGI/Dataset/Training/ISBI2016_ISIC_Part3_Training_Data'

  transform = transforms.Compose([transforms.ToPILImage(),
                                  transforms.AutoAugment(policy = AutoAugmentPolicy.IMAGENET),
                                  transforms.Resize(resolution),
                                  transforms.ToTensor(),
                                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])
  
  dataset = ISIC_Dataset(csv_file = path_csv, img_dir = path_img, transform = transform, data = "Train", device = device)

  return dataset

def stat_dataframe(data_type = None, dataset_path = None):

  df = pd.read_csv(dataset_path)
    
  df_len = len(df.axes[0])


  df_count = df['malignant'].value_counts(normalize=False)

  total_benign = df_count[0]
  total_malignant = df_count[1]
  
  benign_perc = total_benign/df_len * 100
  malignant_perc = total_malignant/df_len * 100

  print("Dataframe: {}\n".format(data_type))

  print("Numero di dati presenti nel dataframe: {}\n".format(df_len))

  print(df)
  print("\n")

  print("Numero di benigni e maligni presenti nel dataframe:")
  print(" - Bengin: {}".format(total_benign))
  print(" - Malignant: {}\n".format(total_malignant))

  print("Percentuale di benigni e maligni nel dataframe:")
  print(" - Benign: {:.3f} %".format(benign_perc))
  print(" - Malignant: {:.3f} %\n".format(malignant_perc))

  print("Numero di valori nulli del dataset:")
  print(" - Colonna 'image-id': {}".format(df.isna().sum()[0]))
  print(" - Colonna 'malignant': {}\n".format(df.isna().sum()[1]))

  print("Risoluzione delle immagini: {}\n".format(resolution))

  sns.histplot(data=df.malignant, discrete=True)
  plt.xlim(0,1)

def stat_dataset(data_type = None, dataset = None):

  num_benign = 0
  num_malignant = 0

  images = []
  labels = []

  for i in range(len(dataset)):
      image, label = dataset.__getitem__(i)
      images.append(image)
      labels.append(label)
      if(label == 0):
        num_benign += 1
      else:
        num_malignant += 1

  print("Dimensione del ", data_type+"_set: ", num_benign+num_malignant, "\n\n")

  print("Numero di nei benigni nel dataset: {}".format(num_benign))
  print("Numero di nei maligni nel dataset: {}\n\n".format(num_malignant))

  print("Percentuale di nei benigni e maligni del dataset:\n")
  print(" - benign: {} %".format(round(num_benign/(num_benign+num_malignant)*100,2)))
  print(" - malignant: {} %".format(round(num_malignant/(num_benign+num_malignant)*100,2)))

def stat_loader(data_type = None, loader = None):
  num_benign = 0
  num_malignant = 0

  for i, data in enumerate(loader, 0):
      inputs, labels = data
      inputs, labels = inputs.to(device), labels.to(device)
      for j in range(labels.size()[0]):
        if(labels[j] == 0):
          num_benign += 1
        else:
          num_malignant += 1

  print("Dimensione del ", data_type+"_set: ", num_benign+num_malignant, "\n\n")

  print("Numero di nei benigni nel dataset: {}".format(num_benign))
  print("Numero di nei maligni nel dataset: {}\n\n".format(num_malignant))

  print("Percentuale di nei benigni e maligni del dataset:\n")
  print(" - benign: {} %".format(round(num_benign/(num_benign+num_malignant)*100,2)))
  print(" - malignant: {} %".format(round(num_malignant/(num_benign+num_malignant)*100,2)))

def print_1batch(dataset = None):

  print("Stampa di 1 batch (batch_size: {}):\n".format(batch_size))
  
  images = []
  labels = []

  for i in range(len(dataset)):
      image, label = dataset.__getitem__(i)
      images.append(image)
      labels.append(label)

  images = random.sample(images, batch_size)
  labels = random.sample(labels, batch_size)

  grid = make_grid(images)
  imshow(grid)

  for i in range(batch_size):
    print("{} Label: {}".format(str(i)  + " " if i<10 else i, classes[labels[i]] + "    " if labels[i] == 0 else classes[labels[i]] + " "))

def imshow(img):
    img[0] = img[0]*0.485 + 0.229     # unnormalize
    img[1] = img[1]*0.456 + 0.224
    img[2] = img[2]*0.406 + 0.225
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

"""##Model, Training and Testing"""

def get_batch_weight(batch_labels = None):
  num_benign = 0
  num_malignant = 0

  batch_labels = batch_labels.type(torch.int64)

  for i in range(len(batch_labels)):
    if(batch_labels[i] == 0):
      num_benign+=1
    else:
      num_malignant+=1

  if num_malignant != 0:

    weight = num_benign/num_malignant
    class_weights = [1, weight]
    batch_weights = [0] * len(batch_labels)

    for i in range(len(batch_labels)):
      class_weight = class_weights[batch_labels[i]]
      batch_weights[i] = class_weight

    batch_weights = torch.Tensor(batch_weights)
  
  else:
    batch_weights = torch.ones(len(batch_labels))
  
  return reshape(batch_weights, (len(batch_labels), 1))

def get_model():
  
  model =  torchvision.models.resnet34(weights='DEFAULT')
  nr_filters = model.fc.in_features
  model.fc = nn.Linear(nr_filters, 1)

  for params in model.parameters():
    params.requires_grad_ = True

  model.to(device)

  criterion = nn.BCELoss()
  criterion.to(device)

  optimizer = optim.Adam(model.parameters(), lr = learning_rate)

  print("Model:     ResNet 34 (pretrained)")
  print("Criterion: BCELoss")
  print("Optimizer: Adam (lr = {})".format(learning_rate))
  
  return model, criterion, optimizer

def train_epoch(model = None, device = None, train_loader = None, optimizer = None):

  loss_epoch = 0
  correct_prediction_epoch = 0
  total_prediction_epoch = 0

  model.train()

  for i, data in enumerate(train_loader, 0):

    inputs, labels = data
    labels = reshape(labels, (labels.size()[0],1)).float()
    inputs, labels = inputs.to(device), labels.to(device)

    optimizer.zero_grad()

    outputs = model(inputs)
    outputs = torch.sigmoid(outputs)

    batch_weights = get_batch_weight(batch_labels = labels)
    criterion = nn.BCELoss(weight = batch_weights)
    criterion.to(device)

    loss = criterion(outputs, labels)
    loss.backward()
    
    optimizer.step()

    loss_epoch += loss.item()

    prediction = []
    target = []

    for j in range(0, labels.size()[0]):
      out = 0 #benign
      if(outputs[j]>=0.5):
        out = 1 #malignant
      
      lab = labels[j]

      prediction.append(out)
      target.append(lab.item())

      if(lab == out):
        correct_prediction_epoch += 1

      total_prediction_epoch += 1
        
  return loss_epoch, correct_prediction_epoch

def validation_epoch(model = None, device = None, val_loader = None, criterion = None, optimizer = None):

  loss_epoch = 0
  correct_prediction_epoch = 0
  total_prediction_epoch = 0

  model.eval()

  with torch.no_grad():

    for i, data in enumerate(val_loader, 0):

      inputs, labels = data
      labels = reshape(labels, (labels.size()[0],1)).float()
      inputs, labels = inputs.to(device), labels.to(device)

      outputs = model(inputs)
      outputs = torch.sigmoid(outputs) 

      loss = criterion(outputs, labels)

      loss_epoch += loss.item()

      for j in range(0, labels.size()[0]):
        out = 0 #benign
        if(outputs[j]>=0.5):
          out = 1 #malignant
          
        lab = labels[j]

        if(lab == out):
          correct_prediction_epoch += 1

        total_prediction_epoch += 1
      
  return loss_epoch, correct_prediction_epoch

def training(dataset = None, model = None, device = None, optimizer = None, criterion = None):

  total_train_loss = []
  total_train_acc = []
  total_val_loss = []
  total_val_acc = []

  print("---------------------------------------------------------------------------------------------------------------------------------")                                     
  print('-------------------------------------------------------- Start Training ---------------------------------------------------------')
  print("---------------------------------------------------------------------------------------------------------------------------------\n\n")

  for fold, (train_idx, val_idx) in enumerate(Kfolder.split(np.arange(len(dataset)))): 

    print("\n------------------------------------------------------------ Fold {} -------------------------------------------------------------\n".format(fold+1))

    train_sampler = SubsetRandomSampler(train_idx)
    val_sampler = SubsetRandomSampler(val_idx)
    train_loader = DataLoader(dataset, batch_size = batch_size, sampler = train_sampler)
    train_aug_loader = DataLoader(get_aug_dataset(), batch_size = batch_size, sampler = train_sampler)
    val_loader = DataLoader(dataset, batch_size = batch_size, sampler = val_sampler)

    model.to(device)

    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}
      
    for epoch in range(num_epochs):

      train_loss, train_correct = train_epoch(model, device, train_loader, optimizer)
      train_aug_loss, train_aug_correct = train_epoch(model, device, train_aug_loader, optimizer)
      val_loss, val_correct = validation_epoch(model, device, val_loader, criterion)

      train_loss = train_loss / len(train_loader.sampler)
      train_acc = train_correct / len(train_loader.sampler) * 100
      val_loss = val_loss / len(val_loader.sampler)
      val_acc = val_correct / len(val_loader.sampler) * 100

      print("Epoch:{}/{} -- Training Loss: {:.3f} - Validation Loss: {:.3f} -- Training Accuracy: {:.2f} % - Validation Accuracy: {:.2f} %".format(epoch+1, num_epochs, train_loss, val_loss, train_acc, val_acc),"\n")

      history['train_loss'].append(train_loss)
      history['test_loss'].append(val_loss)
      history['train_acc'].append(train_acc)
      history['test_acc'].append(val_acc)

      total_train_loss.append(train_loss)
      total_train_acc.append(train_acc)
      total_val_loss.append(val_loss)
      total_val_acc.append(val_acc)

      file = open("/content/drive/MyDrive/Università/Progetto Lab IAGI/Training Model Checkpoint/info_checkpoint.txt", "w")
      file.writelines(['Fold: ',str(fold+1),'\n', 'Epoch: ',str(epoch+1)])
      torch.save(model, "/content/drive/MyDrive/Università/Progetto Lab IAGI/Training Model Checkpoint/checkpoint_model.pt")
      
    fold_performance['fold{}'.format(fold+1)] = history  

    print("\n\n")

  torch.save(model, '/content/drive/MyDrive/Università/Progetto Lab IAGI/model.pt')

  print("\n\n---------------------------------------------------------------------------------------------------------------------------------")                                     
  print('------------------------------------------------------- Finished Training -------------------------------------------------------')
  print("---------------------------------------------------------------------------------------------------------------------------------\n\n")
  
  train_loss_avg, val_loss_avg, train_acc_avg, val_acc_avg = [], [], [], []

  for f in range(1, K+1):

      train_loss_avg.append(np.mean(fold_performance['fold{}'.format(f)]['train_loss']))
      val_loss_avg.append(np.mean(fold_performance['fold{}'.format(f)]['test_loss']))

      train_acc_avg.append(np.mean(fold_performance['fold{}'.format(f)]['train_acc']))
      val_acc_avg.append(np.mean(fold_performance['fold{}'.format(f)]['test_acc']))

  print('Performance of {} fold cross validation:'.format(K))
  print(" - Average Training Loss:       {:.3f}".format(np.mean(train_loss_avg)))
  print(" - Average Validation Loss:     {:.3f}".format(np.mean(val_loss_avg)))
  print(" - Average Training Accuracy:   {:.3f} %".format(np.mean(train_acc_avg)))
  print(" - Average Validation Accuracy: {:.3f} %".format(np.mean(val_acc_avg)))

  return total_train_loss, total_train_acc, total_val_loss, total_val_acc

def test_on_1batch(dataset = None, model = None):

  test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

  dataiter = iter(test_loader)
  images, labels = dataiter.next()

  imshow(torchvision.utils.make_grid(images))
  
  images, labels = images.to(device), labels.to(device)

  outputs = model(images)
  outputs = torch.sigmoid(outputs)

  out = []

  for elem in outputs:
    if elem.item() >= 0.5:
      out.append(1)
    elif elem.item() < 0.5:
      out.append(0)

  print("+------------------------------------+")
  print("| idx  | Ground Truth |  Prediction  |")
  print("|------------------------------------|")
  for i in range(0, batch_size):
    print("| ", i, "  | " if i < 10 else " | ", classes[labels[i]]+"      | " if labels[i] == 0 else classes[labels[i]]+"   | ", classes[out[i]]+"      |" if out[i] == 0 else classes[out[i]]+"   |")
  print("+------------------------------------+")

def test_and_metrics(dataset = None, model = None, total_train_loss = None, total_train_acc = None, total_val_loss = None, total_val_acc = None):
  
  test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
  
  Y = []
  Y_hat = []
  Y_hat_prob = []

  total_train_loss = total_train_loss
  total_train_acc = total_train_acc
  total_val_loss = total_val_loss
  total_val_acc = total_val_acc

  true_benign = 0     # GT = benign and Pred = benign
  false_benign = 0    # GT = malignant and Pred = benign
  true_malignant = 0  # GT = malignant and Pred = malignant
  false_malignant = 0 # GT = benign and Pred = malignant

  sample_weights = []

  with torch.no_grad():
    for data in test_loader:
        images, labels = data
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        outputs = torch.sigmoid(outputs)

        for i in range(0, labels.size()[0]):

          lab = labels[i].item()
          out = 0
          if outputs[i][0].item()>=0.5:
            out = 1

          Y.append(lab)
          Y_hat.append(out)
          Y_hat_prob.append(outputs[i][0].item())

          if(lab == 0 and out == 0):
            true_benign += 1
          elif(lab == 1 and out == 0):
            false_benign += 1
          elif(lab == 1 and out == 1):
            true_malignant += 1
          elif(lab == 0 and out == 1):
            false_malignant += 1

          if(lab == 0):
            sample_weights.append(1)
          else:
            sample_weights.append(4)


  # Learning Curve of Loss and Accuracy
  if total_train_loss != None and total_val_loss != None:
    plt.plot(total_train_loss, label = 'training loss')
    plt.plot(total_val_loss, label = 'validation loss')
    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')
    plt.title("Training and Validation Loss", fontdict={'fontsize':16})
    plt.ylabel("Loss", fontdict={'fontsize':13})
    plt.xlabel("Num_epoch",fontdict={'fontsize':13})
    plt.rc('xtick', labelsize=12)
    plt.rc('ytick', labelsize=12)
    plt.show()

  print("\n")

  if total_train_acc != None and total_val_acc != None:
    plt.plot(total_train_acc, label = 'training accuracy')
    plt.plot(total_val_acc, label = 'validation accuracy')
    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')
    plt.title("Training and Validation Accuracy", fontdict={'fontsize':16})
    plt.ylabel("Accuracy", fontdict={'fontsize':13})
    plt.xlabel("Num_epoch", fontdict={'fontsize':13})
    plt.rc('xtick', labelsize=12)
    plt.rc('ytick', labelsize=12)
    plt.show()

  print("\n\n")


  # Confusion Matrix
  cm = confusion_matrix(Y, Y_hat)
  plt.figure()
  sns.set(font_scale=1)
  hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, yticklabels=['0', '1'], xticklabels=['0', '1'])
  plt.title('Confusion Matrix', fontdict={'fontsize':16})
  plt.ylabel('Ground Truth', fontdict={'fontsize':13})
  plt.xlabel('Prediction', fontdict={'fontsize':13})
  plt.show()

  print('\n')

  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
  plt.figure()
  sns.set(font_scale=1)
  hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, yticklabels=['0', '1'], xticklabels=['0', '1'])
  plt.title('Normalized Confusion Matrix', fontdict={'fontsize':16})
  plt.ylabel('Ground Truth', fontdict={'fontsize':13})
  plt.xlabel('Prediction', fontdict={'fontsize':13})
  plt.show()

  print("\n\n")


  # Roc Curve and AUC
  score_fpr, score_tpr, threshold = roc_curve(Y, Y_hat_prob)
  plt.plot(score_fpr, score_tpr)
  plt.title("ROC Curve", fontdict={'fontsize':16})
  plt.ylabel('True Positive Rate', fontdict={'fontsize':13})
  plt.xlabel('False Positive Rate', fontdict={'fontsize':13})
  plt.rc('xtick', labelsize=12)
  plt.rc('ytick', labelsize=12)
  plt.show()

  print("\n\n")


  # Precision and Recall Curve
  precision, recall, _ = precision_recall_curve(Y, Y_hat_prob)
  plt.plot(recall, precision)
  plt.title("Precision-Recall Curve", fontdict={'fontsize':16})
  plt.ylabel('Precision', fontdict={'fontsize':13})
  plt.xlabel('Recall', fontdict={'fontsize':13})
  plt.rc('xtick', labelsize=12)
  plt.rc('ytick', labelsize=12)
  plt.show()  

  print("\n\n")


  # TPR for AUC for sensitivity > 0.80
  score_tpr_th80 = []
  for tpr in score_tpr:
    if tpr > 0.8:
      score_tpr_th80.append(tpr)
    else:
      score_tpr_th80.append(0)


  # ALL METRICS

  Balanced_Multiclass_Accuracy = balanced_accuracy_score(Y, Y_hat, sample_weight = sample_weights)

  AUC = roc_auc_score(Y, Y_hat_prob)

  AUC_ThSens80 = np.trapz(score_tpr_th80, score_fpr)

  Average_Precision = average_precision_score(Y, Y_hat_prob)

  accuracy = (true_benign+true_malignant)/(true_benign+true_malignant+false_benign+false_malignant)

  accuracy_benign_class = true_benign/(true_benign+false_malignant)

  accuracy_malignant_class = true_malignant/(true_malignant+false_benign)

  specificity = true_benign / (false_malignant + true_benign)

  prevalence = (true_malignant + false_benign) / (true_benign+false_benign+true_malignant+false_malignant)

  sensitivity = true_malignant/(true_malignant+false_benign)

  PPV = (sensitivity * prevalence)/((sensitivity*prevalence)+((1-specificity)*(1-prevalence)))

  NPV = (specificity*(1-prevalence))/(((1-sensitivity)*prevalence) + ((specificity)*1-prevalence))

  F1_score = f1_score(Y, Y_hat)


  print("+------------------------------------------+")
  print("|          ISIC Challenge Metrics          |")
  print("|------------------------------------------|")
  print("|             Metric             |  Value  |")
  print("|--------------------------------|---------|")
  print("|  Balanced Multiclass Accuracy: |  {:.3f}  |".format(Balanced_Multiclass_Accuracy))
  print("|--------------------------------|---------|")
  print("|  AUC:                          |  {:.3f}  |".format(AUC))
  print("|--------------------------------|---------|")
  print("|  AUC, Sens>80%:                |  {:.3f}  |".format(AUC_ThSens80))
  print("|--------------------------------|---------|")
  print("|  Average precision:            |  {:.3f}  |".format(Average_Precision))
  print("|--------------------------------|---------|")
  print("|  Accuracy of model:            |  {:.3f}  |".format(accuracy))
  print("|--------------------------------|---------|")
  print("|  Accuracy of benign class:     |  {:.3f}  |".format(accuracy_benign_class))
  print("|--------------------------------|---------|")
  print("|  Accuracy of malignant class:  |  {:.3f}  |".format(accuracy_malignant_class))
  print("|--------------------------------|---------|")
  print("|  Sensitivity (recall):         |  {:.3f}  |".format(sensitivity))
  print("|--------------------------------|---------|")
  print("|  Specificity:                  |  {:.3f}  |".format(specificity))
  print("|--------------------------------|---------|")
  print("|  Prevalence:                   |  {:.3f}  |".format(prevalence))
  print("|--------------------------------|---------|")
  print("|  F1-Score (Dice Cofficient):   |  {:.3f}  |".format(F1_score))
  print("|--------------------------------|---------|")
  print("|  PPV (precision):              |  {:.3f}  |".format(PPV))
  print("|--------------------------------|---------|")
  print("|  NPV:                          |  {:.3f}  |".format(NPV))
  print("+------------------------------------------+")

"""##Nevi Classification

###Data Visualization and Training
"""

train_set = get_dataset(data_type = "Train",
                        path_csv = '/content/drive/MyDrive/Università/Progetto Lab IAGI/Dataset/Training/ISBI2016_ISIC_Part3_Training_GroundTruth.csv', 
                        path_img = '/content/drive/MyDrive/Università/Progetto Lab IAGI/Dataset/Training/ISBI2016_ISIC_Part3_Training_Data',
                        batch_size = batch_size)

test_set = get_dataset(data_type = "Test",
                        path_csv = '/content/drive/MyDrive/Università/Progetto Lab IAGI/Dataset/Test/ISBI2016_ISIC_Part3_Test_GroundTruth.csv', 
                        path_img = '/content/drive/MyDrive/Università/Progetto Lab IAGI/Dataset/Test/ISBI2016_ISIC_Part3_Test_Data',
                        batch_size = batch_size)

stat_dataframe(data_type = 'Train', dataset_path = '/content/drive/MyDrive/Università/Progetto Lab IAGI/Dataset/Training/ISBI2016_ISIC_Part3_Training_GroundTruth.csv')

stat_dataframe(data_type = 'Test', dataset_path = '/content/drive/MyDrive/Università/Progetto Lab IAGI/Dataset/Test/ISBI2016_ISIC_Part3_Test_GroundTruth.csv')

print_1batch(dataset = train_set)

model, criterion, optimizer = get_model()

total_train_loss, total_train_acc, total_val_loss, total_val_acc = training(dataset = train_set, model = model, device = device, optimizer = optimizer, criterion = criterion)

"""###Testing and Evaluation
-------------------------------------------------------------

I grafici riportati sono:
* Learning curve della training e validation loss;
* Learning curve della training e validation accuracy; 
* Confusion Matrix e Confusion Matrix normalizzata;
* ROC curve;
* Precision-Recall Curve.  

-------------------------------------------------------------

Le metriche utilizzate sono:
* Balanced Multiclass Accuracy;
* AUC: Area under ROC Curve;
* AUC, Sens>80%: AUC calcolata solo per i valori di sensibility > 0.80;
* Avarage Precision: Area under Precision-Recall Curve;
* Accuracy: percentuale predizioni corrette effettuate;
* Accuracy della classe 'benign';
* Accuracy della classe 'malignant';
* Sensitivity (TPR-Recall): numero di casi positivi e predetti come positivi / numero di casi positivi
* Specificity (TNR): numero di casi negativi e predetti come negativi / numero di casi negativi
* Prevalence: numero di casi effettivamente positivi / numero totale di campioni nel dataset
* F1-Score: media armonica tra Precision e Recall;
* PPV (Precision): probabilità che un valore predetto come positivo sia effettivamente positivo;
* NPV: probabilità che un valore predetto come negativo sia effettivamente negativo;
-------------------------------------------------------------




"""

## Load Model: execute ONLY if you want test model after runtime disconnecting ##

#model = torch.load("/content/drive/MyDrive/Università/Progetto Lab IAGI/model.pt", map_location=torch.device('cuda'))
#total_train_loss = None
#total_train_acc = None
#total_val_loss = None
#total_val_acc = None

test_on_1batch(dataset = test_set, model = model)

test_and_metrics(dataset = test_set, model = model, total_train_loss = total_train_loss, total_train_acc = total_train_acc, total_val_loss = total_val_loss, total_val_acc = total_val_acc)

"""Per questa Task (ISIC Challenge 2016, Task 3, Lesion Classification) il primo classificato nella leaderbords ufficale ha riportato i seguenti valori: 

| Metric                      | Value |
|-----------------------------|-------|
| Balanced Multiclass Accuracy| 0.640 |
| AUC                         | 0.804 |
| AUC, Sens>80%               | 0.568 |
| Average Precision           | 0.640 |
| Accuracy                    | 0.855 |
| Sensitivity                 | 0.507 |
| Specificity                 | 0.941 |
| F1-Score (Dice Cofficient)  | 0.580 |
| PPV (precision)             | 0.679 |
| NPV                         | 0.880 |  


*Riferimenti: https://challenge.isic-archive.com/leaderboards/2016/*
"""